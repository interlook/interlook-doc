{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"interlook Automatically configure Load Balancer, (V)IP and DNS for routing ingress traffic to NodePort services deployed on container platform. Features Currently supported Providers: Kubernetes Docker Swarm (Docker Enterprise, CE) Currently supported Provisioners: IP: ipalloc (an embedded simple local IPAM) GestioIP DNS: Consul (DNS records will contain Consul specific suffix: .service. consul-domain , use CoreDNS with rewrite) Load Balancer: Kemp LoadMaster F5 Big-IP LTM","title":"Home"},{"location":"#interlook","text":"Automatically configure Load Balancer, (V)IP and DNS for routing ingress traffic to NodePort services deployed on container platform.","title":"interlook"},{"location":"#features","text":"Currently supported Providers: Kubernetes Docker Swarm (Docker Enterprise, CE) Currently supported Provisioners: IP: ipalloc (an embedded simple local IPAM) GestioIP DNS: Consul (DNS records will contain Consul specific suffix: .service. consul-domain , use CoreDNS with rewrite) Load Balancer: Kemp LoadMaster F5 Big-IP LTM","title":"Features"},{"location":"api/","text":"APIs Basic APIs for viewing interlook information /health Returns HTTP 200 /workflow Returns JSON showing configured workflow steps /services Retuns JSON listing the services { myservice : { wip_time : 0001-01-01T00:00:00Z , state : deployed , expected_state : deployed , time_detected : 2019-09-27T11:32:23.098856973+02:00 , last_update : 2019-09-27T23:28:48.403824618+02:00 , service : { name : myservice , hosts : [ 10.32.2.41 , 10.32.2.46 , 10.32.2.42 ], port : 30004 , public_ip : 10.32.30.2 , dns_name : [ myapp.mydomain.me ] }, close_time : 2019-09-27T21:33:59.774608662+02:00 } } /version Retuns interlook 's version","title":"API"},{"location":"api/#apis","text":"Basic APIs for viewing interlook information","title":"APIs"},{"location":"api/#health","text":"Returns HTTP 200","title":"/health"},{"location":"api/#workflow","text":"Returns JSON showing configured workflow steps","title":"/workflow"},{"location":"api/#services","text":"Retuns JSON listing the services { myservice : { wip_time : 0001-01-01T00:00:00Z , state : deployed , expected_state : deployed , time_detected : 2019-09-27T11:32:23.098856973+02:00 , last_update : 2019-09-27T23:28:48.403824618+02:00 , service : { name : myservice , hosts : [ 10.32.2.41 , 10.32.2.46 , 10.32.2.42 ], port : 30004 , public_ip : 10.32.30.2 , dns_name : [ myapp.mydomain.me ] }, close_time : 2019-09-27T21:33:59.774608662+02:00 } }","title":"/services"},{"location":"api/#version","text":"Retuns interlook 's version","title":"/version"},{"location":"architecture/","text":"Architecture Technically, providers and provisioners are all implementations of the Extension interface. The provisioner watches the container orchestration platform for \"to be deployed\" NodePort services. When a service is detected, it sends a message to the core module. The core keeps track of services state. Based on the current state and message from provider, it will send instruction to the provisioners so that the service gets to the target state.","title":"Architecture"},{"location":"architecture/#architecture","text":"Technically, providers and provisioners are all implementations of the Extension interface. The provisioner watches the container orchestration platform for \"to be deployed\" NodePort services. When a service is detected, it sends a message to the core module. The core keeps track of services state. Based on the current state and message from provider, it will send instruction to the provisioners so that the service gets to the target state.","title":"Architecture"},{"location":"concept/","text":"Concept","title":"Concept"},{"location":"concept/#concept","text":"","title":"Concept"},{"location":"configuration/","text":"Configuration Interlook uses a yaml formatted configuration file. The file contains different sections: core: Configure interlook's core module core : logLevel : DEBUG listenPort : 8080 logFile : stdout # workflowSteps: comma separated succession of extenstions workflowSteps : provider.swarm,ipam.ipalloc,lb.f5ltm # where the workflow entries are saved workflowEntriesFile : ./share/flowentries.db # how often should the workflow controller run workflowActivityLauncherInterval : 3s # how often should the workflow housekeeper run workflowHousekeeperInterval : 60s # close the entry in error if work in progress for longer than serviceWIPTimeout : 90s # remove entries that have been closed for time cleanUndeployedServiceAfter : 10m # trigger a refresh request to provider if service has not been updated since serviceMaxLastUpdated : 90s The other config sections configure the provider and the provisioner(s) . Each component has its own config section. Refer to each extension's doc for configuration reference.","title":"Configuration"},{"location":"configuration/#configuration","text":"Interlook uses a yaml formatted configuration file. The file contains different sections: core: Configure interlook's core module core : logLevel : DEBUG listenPort : 8080 logFile : stdout # workflowSteps: comma separated succession of extenstions workflowSteps : provider.swarm,ipam.ipalloc,lb.f5ltm # where the workflow entries are saved workflowEntriesFile : ./share/flowentries.db # how often should the workflow controller run workflowActivityLauncherInterval : 3s # how often should the workflow housekeeper run workflowHousekeeperInterval : 60s # close the entry in error if work in progress for longer than serviceWIPTimeout : 90s # remove entries that have been closed for time cleanUndeployedServiceAfter : 10m # trigger a refresh request to provider if service has not been updated since serviceMaxLastUpdated : 90s The other config sections configure the provider and the provisioner(s) . Each component has its own config section. Refer to each extension's doc for configuration reference.","title":"Configuration"},{"location":"consul/","text":"Consul Consul can be used as DNS for interlook discovered services. DNS records will contain Consul specific suffix: .service. consul-domain , use CoreDNS with rewrite Configuration `yaml dns: consul: url: http://127.0.0.1:8500 domain: token:","title":"consul"},{"location":"consul/#consul","text":"Consul can be used as DNS for interlook discovered services. DNS records will contain Consul specific suffix: .service. consul-domain , use CoreDNS with rewrite","title":"Consul"},{"location":"consul/#configuration","text":"`yaml dns: consul: url: http://127.0.0.1:8500 domain: token:","title":"Configuration"},{"location":"extension/","text":"Developing an Interlook extension Interface An Interlook extension must first implement the Extension interface: type Extension interface { Start(receive -chan service.Message, send chan - service.Message) error Stop() error } The Start method will be used by the core to start the extension. The core will provide a receive and send channels for the exchanged messages. The Stop method is used to shut the extension down. When invoked, it must make sure that Start method is stopped and return to the invoker. Configuration The configuration is read at interlook startup. The config package must import the extension's configuration object. Interlook configuration is a yaml formatted file and the Go object is config.ServerConfiguration. Here is an example for an IPAM IPAlloc extension: IPAM struct { IPAlloc *ipalloc.Provisioner `yaml: ipalloc,omitempty ` } `yaml: ipam,omitempty ` On startup, interlook will start all extensions that are configured in the core.workflow setup. If a configured extension fails to start, interlook will fail to start. In order to avoid reflexion, the core 's initExtensions contains a map of extensions that needs to be enriched with new extension: knownExt := map[string]Extension{ provider.kubernetes : s.config.Provider.Kubernetes, provider.swarm : s.config.Provider.Swarm, provisioner.consul : s.config.DNS.Consul, provisioner.ipalloc : s.config.IPAM.IPAlloc, provisioner.f5ltm : s.config.LB.F5LTM, provisioner.kemplm : s.config.LB.KempLM, } Messages Two actions must be supported for incoming messages: add : when core sends such a message, it means the extension must create or update the existing service definition delete: service is being un-deployed, so the extension can delete current definition Once processed, the message must be sent back to the core using the send channel. If applicable, the service definition can be modified by the extension. In case of error, the extension must raise it through the Message.Error field.","title":"Extending Interlook"},{"location":"extension/#developing-an-interlook-extension","text":"","title":"Developing an Interlook extension"},{"location":"extension/#interface","text":"An Interlook extension must first implement the Extension interface: type Extension interface { Start(receive -chan service.Message, send chan - service.Message) error Stop() error } The Start method will be used by the core to start the extension. The core will provide a receive and send channels for the exchanged messages. The Stop method is used to shut the extension down. When invoked, it must make sure that Start method is stopped and return to the invoker.","title":"Interface"},{"location":"extension/#configuration","text":"The configuration is read at interlook startup. The config package must import the extension's configuration object. Interlook configuration is a yaml formatted file and the Go object is config.ServerConfiguration. Here is an example for an IPAM IPAlloc extension: IPAM struct { IPAlloc *ipalloc.Provisioner `yaml: ipalloc,omitempty ` } `yaml: ipam,omitempty ` On startup, interlook will start all extensions that are configured in the core.workflow setup. If a configured extension fails to start, interlook will fail to start. In order to avoid reflexion, the core 's initExtensions contains a map of extensions that needs to be enriched with new extension: knownExt := map[string]Extension{ provider.kubernetes : s.config.Provider.Kubernetes, provider.swarm : s.config.Provider.Swarm, provisioner.consul : s.config.DNS.Consul, provisioner.ipalloc : s.config.IPAM.IPAlloc, provisioner.f5ltm : s.config.LB.F5LTM, provisioner.kemplm : s.config.LB.KempLM, }","title":"Configuration"},{"location":"extension/#messages","text":"Two actions must be supported for incoming messages: add : when core sends such a message, it means the extension must create or update the existing service definition delete: service is being un-deployed, so the extension can delete current definition Once processed, the message must be sent back to the core using the send channel. If applicable, the service definition can be modified by the extension. In case of error, the extension must raise it through the Message.Error field.","title":"Messages"},{"location":"extensions/","text":"Extensions","title":"Extensions"},{"location":"extensions/#extensions","text":"","title":"Extensions"},{"location":"f5ltm/","text":"F5 BigIP interlook can configure the f5 BigIP system for routing the traffic to the containerized application. Update mode Config updateMode drives how interlook will update the F5 BigIP. Two modes are currently supported: vs : for each service, interlook will create/maintain a pool member and its associated pool. The pool members will be the docker host(s) actually running the service's containers. policy : you have to specify a \"global\" policy that will contain the rules allowing proper traffic routing. Two policies can be specified, one for HTTP services ( globalHTTPPolicy ) and one for HTTPS services ( globalSSLPolicy ). Example of HTTP rule: Example of HTTPS rule: Configuration f5ltm : httpEndpoint : https://10.32.20.100 username : api password : restaccess authProvider : tmos authToken : httpPort : 80 httpsPort : 443 monitorName : tcp tcpProfile : partition : interlook loadBalancingMode : least-connections-member updateMode : policy globalHTTPPolicy : interlook_http_policy globalSSLPolicy : interlook_https_policy objectDescriptionSuffix :","title":"f5ltm"},{"location":"f5ltm/#f5-bigip","text":"interlook can configure the f5 BigIP system for routing the traffic to the containerized application.","title":"F5 BigIP"},{"location":"f5ltm/#update-mode","text":"Config updateMode drives how interlook will update the F5 BigIP. Two modes are currently supported: vs : for each service, interlook will create/maintain a pool member and its associated pool. The pool members will be the docker host(s) actually running the service's containers. policy : you have to specify a \"global\" policy that will contain the rules allowing proper traffic routing. Two policies can be specified, one for HTTP services ( globalHTTPPolicy ) and one for HTTPS services ( globalSSLPolicy ). Example of HTTP rule: Example of HTTPS rule:","title":"Update mode"},{"location":"f5ltm/#configuration","text":"f5ltm : httpEndpoint : https://10.32.20.100 username : api password : restaccess authProvider : tmos authToken : httpPort : 80 httpsPort : 443 monitorName : tcp tcpProfile : partition : interlook loadBalancingMode : least-connections-member updateMode : policy globalHTTPPolicy : interlook_http_policy globalSSLPolicy : interlook_https_policy objectDescriptionSuffix :","title":"Configuration"},{"location":"ipalloc/","text":"ipalloc IPAlloc is a simple IP allocator. It allocates \"free\" IP address from the configured subnet and keeps track of them is a JSON file. config ipalloc : network_cidr : 192.168.99.0/24 db_file : ./share/allocated.db","title":"ipalloc"},{"location":"ipalloc/#ipalloc","text":"IPAlloc is a simple IP allocator. It allocates \"free\" IP address from the configured subnet and keeps track of them is a JSON file.","title":"ipalloc"},{"location":"ipalloc/#config","text":"ipalloc : network_cidr : 192.168.99.0/24 db_file : ./share/allocated.db","title":"config"},{"location":"kemplm/","text":"Kemp load balancer Configuration lb : kemplm : endpoint : https://192.168.99.2 username : api password : apiPassw0rd httpPort : httpsPort :","title":"kemplm"},{"location":"kemplm/#kemp-load-balancer","text":"","title":"Kemp load balancer"},{"location":"kemplm/#configuration","text":"lb : kemplm : endpoint : https://192.168.99.2 username : api password : apiPassw0rd httpPort : httpsPort :","title":"Configuration"},{"location":"kubernetes/","text":"KUbernetes provider interlook can watch Kubernetes cluster to detect NodePort services that needs to be \"published\". The following labels must be setup at service level for interlook to detect them: interlook.hosts : comma separated list of hosts to be published interlook.port : the application's target port interlook.ssl : boolean, indicates if application is ssl exposed Additional service label(s) can be configured to further filter the interlook service scan. This needs to be configured as labelSelector in the configuration. Configuration provider : kubernetes : endpoint : https://192.168.39.89:6443 labelSelector : - l7aas tlsCa : /path/to/kube/ca.crt tlsCert : /path/to/kube/client.crt tlsKey : /path/to/kube/client.key pollInterval : 15s","title":"Kubernetes"},{"location":"kubernetes/#kubernetes-provider","text":"interlook can watch Kubernetes cluster to detect NodePort services that needs to be \"published\". The following labels must be setup at service level for interlook to detect them: interlook.hosts : comma separated list of hosts to be published interlook.port : the application's target port interlook.ssl : boolean, indicates if application is ssl exposed Additional service label(s) can be configured to further filter the interlook service scan. This needs to be configured as labelSelector in the configuration.","title":"KUbernetes provider"},{"location":"kubernetes/#configuration","text":"provider : kubernetes : endpoint : https://192.168.39.89:6443 labelSelector : - l7aas tlsCa : /path/to/kube/ca.crt tlsCert : /path/to/kube/client.crt tlsKey : /path/to/kube/client.key pollInterval : 15s","title":"Configuration"},{"location":"swarm/","text":"Swarm provider interlook can scan Docker Swarm cluster to detect services that needs to be \"published\". The following labels must be setup at service level for interlook to detect them: interlook.hosts : comma separated list of hosts to be published interlook.port : the application's target port interlook.ssl : boolean, indicates if application is ssl exposed Additional service label(s) can be configured to further filter the interlook scan. This needs to be configured as labelSelector in the configuration. Configuration provider : swarm : endpoint : tcp://ucp.csnet.me:443 labelSelector : - l7aas tlsCa : /home/michael/dkr/bundle/interlook/ca.pem tlsCert : /home/michael/dkr/bundle/interlook/cert.pem tlsKey : /home/michael/dkr/bundle/interlook/key.pem pollInterval : 5s","title":"Swarm"},{"location":"swarm/#swarm-provider","text":"interlook can scan Docker Swarm cluster to detect services that needs to be \"published\". The following labels must be setup at service level for interlook to detect them: interlook.hosts : comma separated list of hosts to be published interlook.port : the application's target port interlook.ssl : boolean, indicates if application is ssl exposed Additional service label(s) can be configured to further filter the interlook scan. This needs to be configured as labelSelector in the configuration.","title":"Swarm provider"},{"location":"swarm/#configuration","text":"provider : swarm : endpoint : tcp://ucp.csnet.me:443 labelSelector : - l7aas tlsCa : /home/michael/dkr/bundle/interlook/ca.pem tlsCert : /home/michael/dkr/bundle/interlook/cert.pem tlsKey : /home/michael/dkr/bundle/interlook/key.pem pollInterval : 5s","title":"Configuration"},{"location":"workflow/","text":"Workflow This doc describes the interlook workflow by explaining a basic example. interlook creates a listener for each configured extension. This listener will receive all messages/events coming from a given extension. Those events, together with their state will be stored in an entry list. In order to bring the services to the desired state (deployed or undeployed), interlook polls the entry list at an interval defined in the config ( core.checkFlowInterval ). It then sends an event to the corresponding extension. The extension is handling the required action and sending back the status to the core listener. Example We have a provider (docker) that publishes services on given host(s) / port When we get a new service definition from docker, we want to get an IP from our IPAM component (local file) In the yml config file, our workflow definition will be: provider.docker,ipam.ipalloc The internal flow/message exchange will be like this: 1. Provider to workflow Docker provider pushes a newly published service. The listener gets it and inject it to the workflow |Provider| - Listener - |workflow| state:provider.docker wip:false 2. The workflowController detects the new entry workflowController check current state against the expected state. If it does not match, sets the next step/extension, changes the status to \"wip\" and sends the message to the next extension (IPAM) |workflow| - flowControl - |IPAM| state:ipam.ipalloc wip:true 3. The IPAM extension sends back a message IPAM extension does it's job and gets an IP for us. Then it sends back the message |IPAM| - Listener - |workflow| state:ipam.file state:ipam.ipalloc wip:true wip:false 4. Closing the flow In our example the ipalloc extension is the last step of the workflow. The flow control module detects that \"ipam.ipalloc\" is the last step in our workflow. As we have reached the final step, it updates the entry's state to deployed, closing the flow. |workflow| state:deployed wip:false","title":"Workflow"},{"location":"workflow/#workflow","text":"This doc describes the interlook workflow by explaining a basic example. interlook creates a listener for each configured extension. This listener will receive all messages/events coming from a given extension. Those events, together with their state will be stored in an entry list. In order to bring the services to the desired state (deployed or undeployed), interlook polls the entry list at an interval defined in the config ( core.checkFlowInterval ). It then sends an event to the corresponding extension. The extension is handling the required action and sending back the status to the core listener.","title":"Workflow"},{"location":"workflow/#example","text":"We have a provider (docker) that publishes services on given host(s) / port When we get a new service definition from docker, we want to get an IP from our IPAM component (local file) In the yml config file, our workflow definition will be: provider.docker,ipam.ipalloc The internal flow/message exchange will be like this:","title":"Example"},{"location":"workflow/#1-provider-to-workflow","text":"Docker provider pushes a newly published service. The listener gets it and inject it to the workflow |Provider| - Listener - |workflow| state:provider.docker wip:false","title":"1. Provider to workflow"},{"location":"workflow/#2-the-workflowcontroller-detects-the-new-entry","text":"workflowController check current state against the expected state. If it does not match, sets the next step/extension, changes the status to \"wip\" and sends the message to the next extension (IPAM) |workflow| - flowControl - |IPAM| state:ipam.ipalloc wip:true","title":"2. The workflowController detects the new entry"},{"location":"workflow/#3-the-ipam-extension-sends-back-a-message","text":"IPAM extension does it's job and gets an IP for us. Then it sends back the message |IPAM| - Listener - |workflow| state:ipam.file state:ipam.ipalloc wip:true wip:false","title":"3. The IPAM extension sends back a message"},{"location":"workflow/#4-closing-the-flow","text":"In our example the ipalloc extension is the last step of the workflow. The flow control module detects that \"ipam.ipalloc\" is the last step in our workflow. As we have reached the final step, it updates the entry's state to deployed, closing the flow. |workflow| state:deployed wip:false","title":"4. Closing the flow"}]}